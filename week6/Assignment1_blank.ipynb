{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"Assignment1_blank.ipynb의 사본","provenance":[{"file_id":"1fu989OXevh09dUBGz-eH1CvP5W5rmYw2","timestamp":1630993604975}],"collapsed_sections":["pcDoom-87xX4"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3.8.5 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"36c7f16063dd4278a35a23f422dfacfb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c1b8f6a02e144380878066f006620f23","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_09b2356c955d446e8ee7f0d7c0944bf9","IPY_MODEL_73132d41942d40d29f79ad9610a0589a","IPY_MODEL_ebf61dadc9c9487b839fe2a859ed2f36"]}},"c1b8f6a02e144380878066f006620f23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"09b2356c955d446e8ee7f0d7c0944bf9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_570859fc4bce46f6ab394094db0a77ce","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_274606c79c8c4f35a8a18378ea06b2fc"}},"73132d41942d40d29f79ad9610a0589a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_807bf459b5364ab599467085c439e67d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":531460341,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":531460341,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9fb519f7cd6d47c18c9bd781ac210415"}},"ebf61dadc9c9487b839fe2a859ed2f36":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_947e1ce288ac49c4972cb947522d7cf4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 507M/507M [00:06&lt;00:00, 102MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_11be7794e0ca4220afaefb24c1bf1571"}},"570859fc4bce46f6ab394094db0a77ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"274606c79c8c4f35a8a18378ea06b2fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"807bf459b5364ab599467085c439e67d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9fb519f7cd6d47c18c9bd781ac210415":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"947e1ce288ac49c4972cb947522d7cf4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"11be7794e0ca4220afaefb24c1bf1571":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"interpreter":{"hash":"98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"}},"cells":[{"cell_type":"markdown","source":["# 필수과제 1 : VGG-11 implementation and fine-tuning\n"],"metadata":{"id":"ScDgQXnt5mfK"}},{"cell_type":"markdown","source":["필수과제 1번에서는 VGG-11 model을 구현해보고, QuickDraw dataset에 대해 직접 구현한 모델을 완전히 random 한 weight에서 시작하는(training from scratch) 학습을 해보게 됩니다. 또, torchvision에 내장된 VGG-11의 pretrained weight를 불러와 적절한 수정을 통해 feature extractor 부분은 고정하고, classification을 수행하는 부분만 새로 학습하는 fine-tuning을 시도하며 training from scratch와의 차이점을 비교하게 됩니다.\n","\n","과제 목표\n","- Pre-trained 네트워크를 활용 할 수 있다.\n","- Fine-tuning을 위해 네트워크의 일부를 고정하고 다른 일부만 학습할 수 있다.\n","- 네트워크를 일부 커스터마이즈해서 다른 문제에 적용할 수 있는 응용 능력을 갖춘다.\n"],"metadata":{"id":"zR-05csy6gzf"}},{"cell_type":"markdown","source":["## Quickdraw dataset\n","Quickdraw dataset은 다양한 물체에 대한 간단한 sketch로 구성된 데이터셋입니다. [데이터셋 홈페이지](https://quickdraw.withgoogle.com/data)에서 예시 이미지들을 확인할 수 있으니, 데이터셋에 어떤 이미지들이 있는지 확인해보시기 바랍니다. \n","\n","이번 과제에서는 model을 보다 쉽게 구현할 수 있도록 데이터를 불러오는 과정에서 모든 이미지들을 VGG-11의 기본 input size인 (224 x 224)로 변경해줍니다. 또, 전체 class가 아닌 10개의 class만을 사용해 학습을 진행하게 됩니다.\n","\n","아래 가려진 코드들은 필요한 패키지들을 import하고, QuickDraw dataset을 사용하기 위한 준비 코드들입니다. 코드가 꽤 길어 가려두었으며, 과제를 수행하기 위해 이해가 필수적인 코드는 아니지만 해당 과정을 이해하고 싶다면 코드를 확인해보셔도 좋습니다.\n"],"metadata":{"id":"pcDoom-87xX4"}},{"cell_type":"code","execution_count":1,"source":["# install quickdraw python API\n","!pip3 install quickdraw"],"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: quickdraw in /opt/conda/lib/python3.8/site-packages (0.2.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from quickdraw) (2.26.0)\n","Requirement already satisfied: pillow in /opt/conda/lib/python3.8/site-packages (from quickdraw) (8.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->quickdraw) (3.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->quickdraw) (1.26.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->quickdraw) (2021.5.30)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->quickdraw) (2.0.4)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XR1bH4WF6qBP","executionInfo":{"status":"ok","timestamp":1629471653287,"user_tz":-540,"elapsed":4758,"user":{"displayName":"‍김종하[ 학부재학 / 컴퓨터학과 ]","photoUrl":"","userId":"09112571090351725461"}},"outputId":"0bf48770-9c1b-4c12-ab81-7fd4d16ce97b"}},{"cell_type":"code","execution_count":2,"source":["# import packages\n","from quickdraw import QuickDrawData, QuickDrawDataGroup\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import itertools\n","import matplotlib.pyplot as plt\n","import os\n","import numpy as np\n","import torch.nn as nn\n","import pandas as pd"],"outputs":[],"metadata":{"id":"04jAy3qh7Lpj"}},{"cell_type":"code","execution_count":3,"source":["# fix random seeds\n","import random\n","random.seed(100)\n","torch.manual_seed(0)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f4b6044fc90>"]},"metadata":{},"execution_count":3}],"metadata":{"id":"MwigL3eK8ZS6"}},{"cell_type":"code","execution_count":4,"source":["num_img_per_class = 3000\n","qd = QuickDrawData(max_drawings=num_img_per_class)"],"outputs":[],"metadata":{"id":"RVpsG6D7IXFq"}},{"cell_type":"code","execution_count":5,"source":["class_list = ['apple', 'wine bottle', 'spoon', 'rainbow', 'panda', 'hospital', 'scissors', 'toothpaste', 'baseball', 'hourglass']\n","class_dict = {'apple' : 0, 'wine bottle' : 1, 'spoon' : 2, 'rainbow' : 3, 'panda': 4, 'hospital' : 5, 'scissors' : 6, 'toothpaste' : 7, 'baseball' : 8, 'hourglass' : 9}"],"outputs":[],"metadata":{"id":"GRk-2HBhBb99"}},{"cell_type":"code","execution_count":6,"source":["qd.load_drawings(class_list)"],"outputs":[{"output_type":"stream","name":"stdout","text":["loading apple drawings\n","load complete\n","loading wine bottle drawings\n","load complete\n","loading spoon drawings\n","load complete\n","loading rainbow drawings\n","load complete\n","loading panda drawings\n","load complete\n","loading hospital drawings\n","load complete\n","loading scissors drawings\n","load complete\n","loading toothpaste drawings\n","load complete\n","loading baseball drawings\n","load complete\n","loading hourglass drawings\n","load complete\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70K8gdG69UI0","executionInfo":{"status":"ok","timestamp":1629471684549,"user_tz":-540,"elapsed":26946,"user":{"displayName":"‍김종하[ 학부재학 / 컴퓨터학과 ]","photoUrl":"","userId":"09112571090351725461"}},"outputId":"f9e5c017-7059-4c71-a25f-5da3f12a1052"}},{"cell_type":"code","execution_count":7,"source":["# get images, and append to train/validation data and label list\n","train_data = list()\n","val_data = list()\n","train_label = list()\n","val_label = list()\n","for class_name in class_list:\n","  qdgroup = QuickDrawDataGroup(class_name, max_drawings=num_img_per_class)\n","  for i, img in enumerate(qdgroup.drawings):\n","    if i < int(0.9 * num_img_per_class):\n","      train_data.append(np.asarray(img.get_image()))\n","      train_label.append(class_dict[class_name])\n","    else:\n","      val_data.append(np.asarray(img.get_image()))\n","      val_label.append(class_dict[class_name])"],"outputs":[{"output_type":"stream","name":"stdout","text":["loading apple drawings\n","load complete\n","loading wine bottle drawings\n","load complete\n","loading spoon drawings\n","load complete\n","loading rainbow drawings\n","load complete\n","loading panda drawings\n","load complete\n","loading hospital drawings\n","load complete\n","loading scissors drawings\n","load complete\n","loading toothpaste drawings\n","load complete\n","loading baseball drawings\n","load complete\n","loading hourglass drawings\n","load complete\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0gqxlZUcCDXx","executionInfo":{"status":"ok","timestamp":1629471695001,"user_tz":-540,"elapsed":10464,"user":{"displayName":"‍김종하[ 학부재학 / 컴퓨터학과 ]","photoUrl":"","userId":"09112571090351725461"}},"outputId":"ac9b9616-a46d-4a96-841b-55ef24adf74f"}},{"cell_type":"code","execution_count":8,"source":["# transformation, image to (224, 224) tensor\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(), \n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                          std=[0.229, 0.224, 0.225])\n","])"],"outputs":[],"metadata":{"id":"GOuZir1wI_Jd"}},{"cell_type":"code","execution_count":9,"source":["# custom dataset for Quickdraw\n","class QuickDrawDataset(Dataset):\n","\n","    def __init__(self, data, labels, transform=None):\n","        self.data = data\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img = self.data[idx]\n","        label = self.labels[idx]\n","        if self.transform:\n","          img = self.transform(img)\n","        return img, label"],"outputs":[],"metadata":{"id":"veN_TBjQ-a-y"}},{"cell_type":"code","execution_count":10,"source":["# quickdraw train/validatoin dataset and dataloader\n","qd_train_dataset = QuickDrawDataset(train_data, train_label, transform)\n","qd_val_dataset = QuickDrawDataset(val_data, val_label, transform)\n","\n","qd_train_dataloader = DataLoader(qd_train_dataset, batch_size=64, shuffle=True)\n","qd_val_dataloader = DataLoader(qd_val_dataset, batch_size=64, shuffle=True)"],"outputs":[],"metadata":{"id":"T6ooe0ZYAIgP"}},{"cell_type":"code","execution_count":11,"source":["# Misc\n","\n","class AverageMeter(object):\n","  \"\"\"Computes and stores the average and current value\"\"\"\n","  def __init__(self):\n","      self.reset()\n","\n","  def reset(self):\n","    self.val = 0\n","    self.avg = 0\n","    self.sum = 0\n","    self.count = 0\n","\n","  def update(self, val, n=1):\n","    self.val = val\n","    self.sum += val * n\n","    self.count += n\n","    self.avg = self.sum / self.count"],"outputs":[],"metadata":{"id":"YmhkezJwDq4W"}},{"cell_type":"markdown","source":["## Custom VGG-11 implementation\n","\n","아래 Table은 여러 종류의 VGG 네트워크에 대한 각각의 구성을 나타낸 것입니다.\n","<br>A에서 E까지 다양한 크기의 VGG 중에서 구현하고자 하는 네트워크는 **A에 해당하는 VGG-11**입니다.\n","\n","----\n","\n","<img src='https://drive.google.com/uc?id=1bFKnmwcbdJLQdCjNPxQdaYnTNCmBt0rK'  width=\"700\">\n","\n","아래 코드는 VGG-11 네트워크를 나타내는 class입니다. 위 figure를 참고해서 코드의 TODO 부분을 채워주세요. 필요한 경우, [VGG-11 original paper](https://arxiv.org/pdf/1409.1556.pdf)를 참고하셔도 좋습니다.\n","\n","- **TO DO (1-1)** : Feature extractor 역할을 수행하는 convolution layers를 구현하는 것이 과제입니다. 주어진 ```self.conv1, self.bn, self.pool1```코드와 위 표를 참고하여 나머지 부분을 채워주세요.\n","\n","- **TO DO (1-2)** : Convolution filter로 이루어진 feature extractor에서 얻은 이미지에 대한 feature를 통해 classification task를 수행하는 fully connected layer를 구현하고자 합니다. Convolution layer의 output dimension을 고려해, convolution layer에서 extract된 feature들이 Quickdraw의 **10개 class**에 해당하는 output으로 잘 연결될 수 있도록 fully connected layer의 일부를 채워주세요. \n","\n","- TO DO (1-2)의 힌트로, forward 함수의 feature extraction이 끝나고 FC layer가 시작하는 부분의 output tensor shape를 확인할 수 있는 코드를 작성해두었습니다. Model의 feed forward를 수행하는 과정에서 해당 부분의 tensor shape가 출력되므로, 이 정보를 이용해 fully connected layer의 input dimension을 계산 할 수 있습니다. 이처럼, 모델을 만드는 과정에서 복잡한 기술 없이 간단한 print()문 하나만으로도 큰 도움을 얻을 수 있다는 점을 기억하고 있으시면 큰 도움이 될 것이라 생각합니다. VGG 모델을 모두 구현하고 나서는 forward 과정에서 원치 않는 tensor shape의 출력을 방지하기 위해 해당 부분 코드를 주석처리 하거나 삭제하시면 됩니다."],"metadata":{"id":"MFTw6h-P-JxY"}},{"cell_type":"code","execution_count":12,"source":["import torch\n","import torch.nn as nn\n","\n","# 224 x 224 RGB Images\n","# 64 Max 128 Max 256 256 Max 512 512 Max 512 512 Max\n","class VGG11(nn.Module):\n","  def __init__(self, num_classes=10):\n","    super(VGG11, self).__init__()\n","\n","    self.num_classes = num_classes\n","    self.relu = nn.ReLU(inplace=True)\n","    \n","    # Convolution Feature Extraction Part\n","    # In channels: 3\n","    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n","    self.bn1   = nn.BatchNorm2d(64)\n","    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","\n","    # TO DO (1-1) starts here!\n","    # Fill empty parts with proper code\n","    self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","    self.bn2   = nn.BatchNorm2d(128)\n","    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n","    self.bn3_1   = nn.BatchNorm2d(256)\n","    self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n","    self.bn3_2   = nn.BatchNorm2d(256)\n","    self.pool3   = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n","    self.bn4_1   = nn.BatchNorm2d(512)\n","    self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","    self.bn4_2   = nn.BatchNorm2d(512)\n","    self.pool4   = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","    self.bn5_1   = nn.BatchNorm2d(512)\n","    self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","    self.bn5_2   = nn.BatchNorm2d(512)\n","    self.pool5   = nn.MaxPool2d(kernel_size=2, stride=2)\n","    # TO DO (1-1) ends here!\n","\n","    # Fully Connected Classifier Part\n","    # TO DO (1-2) starts here!\n","    # Fill empty parts with proper code\n","    self.fc1      = nn.Sequential(\n","      nn.Linear(512*7*7, 4096),\n","      nn.ReLU()\n","    )\n","    self.dropout1 = nn.Dropout()\n","    self.fc2      = nn.Sequential(\n","      nn.Linear(4096, 4096),\n","      nn.ReLU()\n","    )\n","    self.dropout2 = nn.Dropout()\n","    \n","    self.fc3      = nn.Linear(4096, self.num_classes)\n","    # TO DO (1-2) ends here!\n","\n","  def forward(self, x):\n","    # Convolution Feature Extraction Part\n","    x = self.conv1(x)\n","    x = self.bn1(x)\n","    x = self.relu(x)\n","    x = self.pool1(x)\n","\n","    x = self.conv2(x)\n","    x = self.bn2(x)\n","    x = self.relu(x)\n","    x = self.pool2(x)\n","\n","    x = self.conv3_1(x)\n","    x = self.bn3_1(x)\n","    x = self.relu(x)\n","    x = self.conv3_2(x)\n","    x = self.bn3_2(x)\n","    x = self.relu(x)\n","    x = self.pool3(x)\n","\n","    x = self.conv4_1(x)\n","    x = self.bn4_1(x)\n","    x = self.relu(x)\n","    x = self.conv4_2(x)\n","    x = self.bn4_2(x)\n","    x = self.relu(x)\n","    x = self.pool4(x)\n","\n","    x = self.conv5_1(x)\n","    x = self.bn5_1(x)\n","    x = self.relu(x)\n","    x = self.conv5_2(x)\n","    x = self.bn5_2(x)\n","    x = self.relu(x)\n","    x = self.pool5(x)\n","\n","\n","    # part to help debugging by checking tensor size\n","    # delete or comment out this line after debugging\n","    # print(f'Tensor shape after convolution layers is {x.shape}')\n","\n","    x = torch.flatten(x, 1)\n","    x = self.fc1(x)\n","    x = self.relu(x)\n","    x = self.dropout1(x)\n","    \n","    x = self.fc2(x)\n","    x = self.relu(x)\n","    x = self.dropout2(x)\n","    \n","    x = self.fc3(x)\n","    return x"],"outputs":[],"metadata":{"id":"e2nqVINpDTWk"}},{"cell_type":"markdown","source":["----\n","구현한 VGG-11에 문제가 없다면 네트워크의 **output tensor는 10-dim vector**에 해당해야 합니다. \n","\n","<br>아래의 코드를 실행하여 **output tensor의 shape을 확인**해봅시다.\n","<br> 만약 ```out = model_test(x)```를 실행하였을 때 오류가 발생한다면 layer 구현에 오류가 있는 것이니 다시 한번 네트워크 코드에 문제가 없는지 확인해보세요 :)\n","\n","\n","아래의 문구와 동일하게 출력된다면 output tensor의 shape이 알맞게 return된 것입니다!\n","```python\n","\"Output tensor shape is : torch.Size([1, 10])\"\n","```"],"metadata":{"id":"T9h01uUSA2yU"}},{"cell_type":"code","execution_count":13,"source":["# Network\n","model_test = VGG11(num_classes=10)\n","\n","# Random input\n","x = torch.randn((1, 3, 224, 224))\n","\n","# Forward\n","out = model_test(x)\n","\n","# Check the output shape\n","print(\"Output tensor shape is :\", out.shape)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Output tensor shape is : torch.Size([1, 10])\n"]},{"output_type":"stream","name":"stderr","text":["/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fQbEn0zbBBES","executionInfo":{"status":"ok","timestamp":1629471915291,"user_tz":-540,"elapsed":1911,"user":{"displayName":"‍김종하[ 학부재학 / 컴퓨터학과 ]","photoUrl":"","userId":"09112571090351725461"}},"outputId":"719267a1-f375-4f1e-b552-465acb3bced8"}},{"cell_type":"markdown","source":["## Training custom VGG-11 from scratch\n","이제 직접 구현한 VGG-11을 학습시켜보겠습니다. 모델을 학습하기 위해,\n","1. DataLoader에서 image, label을 불러오고\n","2. model에 image를 foward한 이후 output logit과 label의 loss를 계산하고\n","3. loss를 통해 optimizer의 backward를 수행해 parameter를 업데이트하는\n","\n","일반적인 학습 loop을 완성하게 됩니다.\n","\n","해당 과정에서 과제로 작성해야하는 부분은 아래와 같습니다.\n","\n","- **TO DO (2-1)** : VGG-11을 본격적으로 학습하는 과정입니다. 주석에 적힌 내용을 따라 loss function인 ```criterion```과 ```optimizer```를 활용하여 빈 부분을 채워주세요.\n","\n","- **TO DO (2-2)** : 학습된 VGG-11을 validation dataset에 대해 평가하는 과정입니다. Validation 과정에서는 <U>gradient 계산과 backpropagation이 필요 없다</U>는 것에 주목하여 빈 부분을 채워주세요.\n","\n","- **TO DO (2-3)** : 빠른 학습을 위해서는 GPU 사용이 필수적입니다. GPU에서 학습하기 위해 필요한 것들(모델이나 텐서 등)을 .cuda()명령어를 이용해 적절히 GPU로 옮겨 학습할 수 있게 해주세요.\n","\n","TO DO (2-3)는 다양한 구현 방법이 있을 수 있기 때문에, *starts here, ends here* 주석이 없습니다. 본인이 생각하기에 적절한 위치나, 빈칸으로 남아있는 부분에 구현해주세요."],"metadata":{"id":"2X_GsdEWBdLk"}},{"cell_type":"code","execution_count":14,"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'device: {device}')"],"outputs":[{"output_type":"stream","name":"stdout","text":["device: cuda\n"]}],"metadata":{}},{"cell_type":"code","execution_count":15,"source":["# Build user-defined VGG model\n","model_scratch = VGG11(num_classes=10).to(device)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PHhRYP4ZBI0p","executionInfo":{"status":"ok","timestamp":1629471921296,"user_tz":-540,"elapsed":1711,"user":{"displayName":"‍김종하[ 학부재학 / 컴퓨터학과 ]","photoUrl":"","userId":"09112571090351725461"}},"outputId":"189a15cc-1b5f-4726-ad4c-d7cba3a8f4fb"}},{"cell_type":"code","execution_count":16,"source":["# Loss function and Optimizer\n","from torch.optim import Adam\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = Adam(model_scratch.parameters(), lr=1e-4)"],"outputs":[],"metadata":{"id":"nu1bm42vDtb_"}},{"cell_type":"code","execution_count":17,"source":["log_dir ='./log'"],"outputs":[],"metadata":{"id":"CKZTQB1kH1Dh"}},{"cell_type":"code","execution_count":18,"source":["# Main\n","os.makedirs(log_dir, exist_ok=True)\n","\n","with open(os.path.join(log_dir, 'scratch_train_log.csv'), 'w') as log:\n","  # Training\n","  # TO DO (2-1) starts here!\n","  # Fill empty parts with proper code\n","\n","  # set model to train mode\n","  model_scratch.train()\n","  for iter, (img, label) in enumerate(qd_train_dataloader):\n","\n","    # optimizer에 저장된 미분값을 0으로 초기화\n","    optimizer.zero_grad()\n","\n","    # 학습에 사용하기 위한 image, label 처리 (필요한 경우, data type도 변경해주세요)\n","    img, label = img.to(device), label.to(device)\n","  \n","    # # 모델에 이미지 forward\n","    pred_logit = model_scratch(img)\n","\n","    # # loss 값 계산\n","    loss = criterion(pred_logit, label)\n","\n","    # Backpropagation\n","    loss.backward()\n","    optimizer.step()\n","    # # TO DO (2-1) ends here!\n","\n","    # Accuracy 계산\n","    pred_label = torch.argmax(pred_logit, 1)\n","    acc = (pred_label == label).sum().item() / len(img)\n","\n","    train_loss = loss.item()\n","    train_acc = acc\n","\n","    # Validation \n","    if (iter % 20 == 0) or (iter == len(qd_train_dataloader)-1):\n","      # TO DO (2-2) starts here!\n","      # Fill empty parts with proper code\n","\n","      # set model to validation mode\n","      model_scratch.eval()\n","      valid_loss, valid_acc = AverageMeter(), AverageMeter()\n","\n","      for img, label in qd_val_dataloader:\n","        # Validation에 사용하기 위한 image, label 처리 (필요한 경우, data type도 변경해주세요)\n","        img, label = img.to(device), label.to(device)\n","\n","        # 모델에 이미지 forward (gradient 계산 X)\n","        with torch.no_grad():\n","          pred_logit = model_scratch(img)\n","\n","        # loss 값 계산\n","        loss = criterion(pred_logit, label)\n","\n","        # TO DO (2-2) ends here!\n","\n","        # Accuracy 계산\n","        pred_label = torch.argmax(pred_logit, 1)\n","        acc = (pred_label == label).sum().item() / len(img)\n","\n","        valid_loss.update(loss.item(), len(img))\n","        valid_acc.update(acc, len(img))\n","\n","      valid_loss = valid_loss.avg\n","      valid_acc = valid_acc.avg\n","\n","      print(\"Iter [%3d/%3d] | Train Loss %.4f | Train Acc %.4f | Valid Loss %.4f | Valid Acc %.4f\" %\n","            (iter, len(qd_train_dataloader), train_loss, train_acc, valid_loss, valid_acc))\n","      \n","      # Train Log Writing\n","      log.write('%d,%.4f,%.4f,%.4f,%.4f\\n'%(iter, train_loss, train_acc, valid_loss, valid_acc))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Iter [  0/422] | Train Loss 2.3485 | Train Acc 0.0938 | Valid Loss 2.3027 | Valid Acc 0.1000\n","Iter [ 20/422] | Train Loss 2.3048 | Train Acc 0.0781 | Valid Loss 2.2847 | Valid Acc 0.1163\n","Iter [ 40/422] | Train Loss 1.9205 | Train Acc 0.5000 | Valid Loss 1.8238 | Valid Acc 0.4307\n","Iter [ 60/422] | Train Loss 1.3773 | Train Acc 0.5312 | Valid Loss 1.3590 | Valid Acc 0.5680\n","Iter [ 80/422] | Train Loss 1.1750 | Train Acc 0.5938 | Valid Loss 1.0293 | Valid Acc 0.6760\n","Iter [100/422] | Train Loss 0.7367 | Train Acc 0.7656 | Valid Loss 0.8272 | Valid Acc 0.7457\n","Iter [120/422] | Train Loss 0.7391 | Train Acc 0.7031 | Valid Loss 0.8572 | Valid Acc 0.7197\n","Iter [140/422] | Train Loss 0.7105 | Train Acc 0.7188 | Valid Loss 0.6763 | Valid Acc 0.7963\n","Iter [160/422] | Train Loss 0.7246 | Train Acc 0.7969 | Valid Loss 0.6071 | Valid Acc 0.8233\n","Iter [180/422] | Train Loss 0.6116 | Train Acc 0.8125 | Valid Loss 0.5761 | Valid Acc 0.8310\n","Iter [200/422] | Train Loss 0.4152 | Train Acc 0.8125 | Valid Loss 0.5357 | Valid Acc 0.8320\n","Iter [220/422] | Train Loss 0.3248 | Train Acc 0.9219 | Valid Loss 0.5581 | Valid Acc 0.8363\n","Iter [240/422] | Train Loss 0.4768 | Train Acc 0.8438 | Valid Loss 0.5108 | Valid Acc 0.8500\n","Iter [260/422] | Train Loss 0.7781 | Train Acc 0.7656 | Valid Loss 0.5066 | Valid Acc 0.8460\n","Iter [280/422] | Train Loss 0.4691 | Train Acc 0.8281 | Valid Loss 0.4624 | Valid Acc 0.8580\n","Iter [300/422] | Train Loss 0.5702 | Train Acc 0.8125 | Valid Loss 0.4513 | Valid Acc 0.8597\n","Iter [320/422] | Train Loss 0.3543 | Train Acc 0.9062 | Valid Loss 0.4194 | Valid Acc 0.8740\n","Iter [340/422] | Train Loss 0.4926 | Train Acc 0.8281 | Valid Loss 0.4518 | Valid Acc 0.8647\n","Iter [360/422] | Train Loss 0.3409 | Train Acc 0.9062 | Valid Loss 0.4142 | Valid Acc 0.8830\n","Iter [380/422] | Train Loss 0.4862 | Train Acc 0.8281 | Valid Loss 0.4749 | Valid Acc 0.8550\n","Iter [400/422] | Train Loss 0.3743 | Train Acc 0.8906 | Valid Loss 0.3981 | Valid Acc 0.8840\n","Iter [420/422] | Train Loss 0.4677 | Train Acc 0.8750 | Valid Loss 0.4585 | Valid Acc 0.8597\n","Iter [421/422] | Train Loss 0.5126 | Train Acc 0.8214 | Valid Loss 0.4322 | Valid Acc 0.8707\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fm5DWDr9BPax","executionInfo":{"status":"ok","timestamp":1629472907129,"user_tz":-540,"elapsed":981290,"user":{"displayName":"‍김종하[ 학부재학 / 컴퓨터학과 ]","photoUrl":"","userId":"09112571090351725461"}},"outputId":"f26c6e7d-17f1-46ec-aaee-3425f5953b0b"}},{"cell_type":"markdown","source":["## Fine-tuning VGG-11 with pretrained feature extractor\n","이번에는 torchvision에서 제공하는 pretrained VGG-11을 이용한 fine-tuning 학습을 진행하게 됩니다. Fine-tuning의 경우도 기본적인 training 방법은 동일하지만, 이번에는 모든 parameter를 처음부터 학습하는 대신 **feature extractor는 기존 학습된 상태에서 고정하고 linear classifier만 새로 학습**한다는 차이가 있습니다. \n","\n","Fine tuning을 위해 작성해야하는 코드는 아래와 같습니다.\n","\n","- **TO DO (3-1, 3-2, 3-3)** : Training from scratch의 1~3과 같이, 모델이 잘 학습될 수 있도록 빈 부분을 채워주세요. 마찬가지로, validation을 수행할 때는 gradient 계산 및 backpropagation이 필요 없음을 주목하고, 필요한 경우 model이나 tensor등을 .cuda()를 사용해 적절히 GPU로 옮겨주세요\n","\n","- **TO DO (3-4)** : Torchvision에서 제공하는 VGG-11의 경우는 1000개의 class에 대한 classification을 수행하는 모델입니다. 따라서, Quickdraw dataset에 사용하기 위해서는 마지막 linear layer가 1000-dim vector 대신 10-dim vector를 내보내야 합니다. 이를 위해 마지막 *nn.Linear* layer를 수정해주세요.\n","\n","- **TO DO (3-5)** : Linear classifier만을 학습하기 위해, convolution layers들의 weight를 freeze 해주세요.\n","\n","- Hint : Google에 *how to freeze conv layers* 를 검색한 [결과](https://www.google.com/search?q=how+to+freeze+conv+layers&oq=how+to+&aqs=chrome.0.69i59j69i57j0i512l3j69i60l3.809j0j7&sourceid=chrome&ie=UTF-8) 나오는 [stackoverflow 글](https://discuss.pytorch.org/t/how-the-pytorch-freeze-network-in-some-layers-only-the-rest-of-the-training/7088)을 참고하면 쉽게 구현할 수 있습니다. 이와 같이, 필요한 내용을 잘 googling하는 실력은 앞으로의 개발 과정에서도 필수적인 능력이 될 것입니다. 앞으로를 위해 **정확한 키워드로 잘 검색하는 습관**을 들이시면 모든 함수를 외우지 않고도 효율적이고 빠른 코딩이 가능할 것입니다."],"metadata":{"id":"NuC7ESBFDD8R"}},{"cell_type":"code","execution_count":19,"source":["from torchvision.models import vgg11\n","\n","pretrained = True \n","model_finetune = vgg11(pretrained)"],"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg11-bbd30ac9.pth\" to /opt/ml/.cache/torch/hub/checkpoints/vgg11-bbd30ac9.pth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c13591ad04e468e834ed675d20bf054"},"text/plain":["  0%|          | 0.00/507M [00:00<?, ?B/s]"]},"metadata":{}}],"metadata":{}},{"cell_type":"code","execution_count":31,"source":["list(model_finetune.features.children())[3]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"]},"metadata":{},"execution_count":31}],"metadata":{}},{"cell_type":"code","execution_count":33,"source":["from torchvision.models import vgg11\n","\n","pretrained = True \n","model_finetune = vgg11(pretrained)\n","\n","# TO DO (3-4, 3-5) starts here\n","# Fill empty parts with proper code\n","\n","# change VGG-11's output dimension to 10\n","model_finetune.classifier[6] = nn.Linear(4096, 10)\n","\n","# Freeze the feature extracting convolution layers\n","for param in model_finetune.parameters():\n","    param.requires_grad = False\n","\n","# FILL HERE #\n","\n","## TO DO (3-4, 3-5) ends here"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["36c7f16063dd4278a35a23f422dfacfb","c1b8f6a02e144380878066f006620f23","09b2356c955d446e8ee7f0d7c0944bf9","73132d41942d40d29f79ad9610a0589a","ebf61dadc9c9487b839fe2a859ed2f36","570859fc4bce46f6ab394094db0a77ce","274606c79c8c4f35a8a18378ea06b2fc","807bf459b5364ab599467085c439e67d","9fb519f7cd6d47c18c9bd781ac210415","947e1ce288ac49c4972cb947522d7cf4","11be7794e0ca4220afaefb24c1bf1571"]},"id":"UMqSkZF9ERRR","executionInfo":{"status":"ok","timestamp":1629472921134,"user_tz":-540,"elapsed":8056,"user":{"displayName":"‍김종하[ 학부재학 / 컴퓨터학과 ]","photoUrl":"","userId":"09112571090351725461"}},"outputId":"e8aeeeaf-0c0d-47f5-8cd7-e7ff3cb37183"}},{"cell_type":"code","execution_count":34,"source":["# Loss function and Optimizer\n","from torch.optim import Adam\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer_ft = Adam(model_finetune.parameters(), lr=1e-4)"],"outputs":[],"metadata":{"id":"f9WrU6AfVdyi"}},{"cell_type":"code","execution_count":35,"source":["# Main\n","os.makedirs(log_dir, exist_ok=True)\n","\n","with open(os.path.join(log_dir, 'scratch_train_log.csv'), 'w') as log:\n","  # Training\n","  # TO DO (3-1) starts here!\n","  # Fill empty parts with proper code\n","\n","  # set model to train mode\n","  model_finetune.train()#FILL HERE#\n","  for iter, (img, label) in enumerate(qd_train_dataloader):\n","\n","    # optimizer에 저장된 미분값을 0으로 초기화\n","    optimizer.zero_grad()\n","\n","    # 학습에 사용하기 위한 image, label 처리 (필요한 경우, data type도 변경해주세요)\n","    img, label = img.to(device), label.to(device)\n","  \n","    # 모델에 이미지 forward\n","    pred_logit = model_scratch(img)\n","\n","    # loss 값 계산\n","    loss = criterion(pred_logit, label)\n","\n","    # Backpropagation\n","    loss.backward()\n","    optimizer.step()\n","    # TO DO (3-1) ends here!\n","\n","    # Accuracy 계산\n","    pred_label = torch.argmax(pred_logit, 1)\n","    acc = (pred_label == label).sum().item() / len(img)\n","\n","    train_loss = loss.item()\n","    train_acc = acc\n","\n","    # Validation \n","    if (iter % 20 == 0) or (iter == len(qd_train_dataloader)-1):\n","      # TO DO (3-2) starts here!\n","      # Fill empty parts with proper code\n","\n","      # set model to validation mode\n","      model_finetune.eval()\n","      valid_loss, valid_acc = AverageMeter(), AverageMeter()\n","\n","      for img, label in qd_val_dataloader:\n","        # Validation에 사용하기 위한 image, label 처리 (필요한 경우, data type도 변경해주세요)\n","        img, label = img.to(device), label.to(device)\n","\n","        # 모델에 이미지 forward (gradient 계산 X)\n","        with torch.no_grad():\n","          pred_logit = model_scratch(img)\n","\n","        # loss 값 계산\n","        loss = criterion(pred_logit, label)\n","\n","        # TO DO (3-2) ends here!\n","\n","        # Accuracy 계산\n","        pred_label = torch.argmax(pred_logit, 1)\n","        acc = (pred_label == label).sum().item() / len(img)\n","\n","        valid_loss.update(loss.item(), len(img))\n","        valid_acc.update(acc, len(img))\n","\n","      valid_loss = valid_loss.avg\n","      valid_acc = valid_acc.avg\n","\n","      print(\"Iter [%3d/%3d] | Train Loss %.4f | Train Acc %.4f | Valid Loss %.4f | Valid Acc %.4f\" %\n","            (iter, len(qd_train_dataloader), train_loss, train_acc, valid_loss, valid_acc))\n","      \n","      # Train Log Writing\n","      log.write('%d,%.4f,%.4f,%.4f,%.4f\\n'%(iter, train_loss, train_acc, valid_loss, valid_acc))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Iter [  0/422] | Train Loss 0.2969 | Train Acc 0.8750 | Valid Loss 0.4074 | Valid Acc 0.8790\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zA8Kwg4MHfjE","executionInfo":{"status":"ok","timestamp":1629473616925,"user_tz":-540,"elapsed":688540,"user":{"displayName":"‍김종하[ 학부재학 / 컴퓨터학과 ]","photoUrl":"","userId":"09112571090351725461"}},"outputId":"9ac84729-016c-4509-c226-d5b447fa3dee"}},{"cell_type":"markdown","source":["## Visualizing training results\n","아래 코드는 training from scratch와 fine tuning의 학습 경향을 비교할 수 있는 그래프를 출력해주는 코드입니다. 수렴 속도, accuracy 등 학습 결과에서 확인할 수 있는 정보들을 통해 from scratch, fine tuning중 어떤 방법이 더 우수한지, 또 그 이유는 무엇인지 자유롭게 생각해보고 캠퍼분들과 논의해보세요!"],"metadata":{"id":"ghBPEKbIEiq-"}},{"cell_type":"code","execution_count":null,"source":["# Load log file\n","scratch_train_log = pd.read_csv(os.path.join(log_dir, 'scratch_train_log.csv'), index_col=0, header=None)\n","fine_tuned_train_log = pd.read_csv(os.path.join(log_dir, 'fine_tuned_train_log.csv'), index_col=0, header=None)"],"outputs":[],"metadata":{"id":"DUUzXL47E5Fa"}},{"cell_type":"code","execution_count":null,"source":["# Visualize training log\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,8))\n","\n","ax1.plot(scratch_train_log.iloc[:,0], label='Scratch Training')\n","ax1.plot(fine_tuned_train_log.iloc[:,0], label='Fine Tuning')\n","ax1.set_title('Training Loss Graph', fontsize=15)\n","ax1.set_xlabel('Iteration', fontsize=15)\n","ax1.set_ylabel('Loss', fontsize=15)\n","\n","fig.legend(fontsize=15)\n","\n","ax2.plot(scratch_train_log.iloc[:,1], label='Scratch Training')\n","ax2.plot(fine_tuned_train_log.iloc[:,1], label='Fine Tuning')\n","ax2.set_title('Training Accuracy Graph', fontsize=15)\n","ax2.set_xlabel('Iteration', fontsize=15)\n","ax2.set_ylabel('Accuracy', fontsize=15)\n","\n","plt.show()"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":563},"id":"jv_1nfnGE7so","executionInfo":{"status":"ok","timestamp":1629473647327,"user_tz":-540,"elapsed":1074,"user":{"displayName":"‍김종하[ 학부재학 / 컴퓨터학과 ]","photoUrl":"","userId":"09112571090351725461"}},"outputId":"61327e91-2318-45ce-8e33-8ba5a5a097d2"}},{"cell_type":"code","execution_count":null,"source":["# Visualize validation log\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,8))\n","\n","ax1.plot(scratch_train_log.iloc[:,2], label='Scratch Training')\n","ax1.plot(fine_tuned_train_log.iloc[:,2], label='Fine Tuning')\n","ax1.set_title('Validation Loss Graph', fontsize=15)\n","ax1.set_xlabel('Iteration', fontsize=15)\n","ax1.set_ylabel('Loss', fontsize=15)\n","\n","fig.legend(fontsize=15)\n","\n","ax2.plot(scratch_train_log.iloc[:,3], label='Scratch Training')\n","ax2.plot(fine_tuned_train_log.iloc[:,3], label='Fine Tuning')\n","ax2.set_title('Validation Accuracy Graph', fontsize=15)\n","ax2.set_xlabel('Iteration', fontsize=15)\n","ax2.set_ylabel('Accuracy', fontsize=15)\n","\n","plt.show()"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":563},"id":"LYhBa098E9C2","executionInfo":{"status":"ok","timestamp":1629473656943,"user_tz":-540,"elapsed":1748,"user":{"displayName":"‍김종하[ 학부재학 / 컴퓨터학과 ]","photoUrl":"","userId":"09112571090351725461"}},"outputId":"3c94fc72-7c03-4561-8e97-101221424e65"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}}]}